# -*- coding: utf-8 -*-
"""Finetuning FinBert Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Od1J7bSwLCOdR22N2qK9yR4XdjOaeBm1
"""

import torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import Trainer
from transformers import TrainingArguments

from datasets import Dataset

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split

import pandas as pd
import numpy as np

tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert", num_labels=3)

model.config.id2label

tokenizer.save_pretrained('/content/drive/MyDrive/Tokenizer')

# Move model to GPU if available
device = torch.device(
    "cuda") if torch.cuda.is_available() else torch.device("cpu")
# model = model.to(device)
model = model.to(device)

# Dataframe 1 is based on crypto news dataset
df = pd.read_csv("/CryptoNewsDataset")
df=df.rename(columns={"class": "label"})
df["text"] = df["title"] + df["text"]
df=df.drop(["date", "sentiment", "source", "subject", "title"], axis=1)

train_texts, test_texts = train_test_split(df, stratify=df["label"], test_size=0.1, random_state=42)
train_texts, val_texts = train_test_split(train_texts, stratify=train_texts["label"],test_size=0.1, random_state=42)

dataset_train = Dataset.from_pandas(train_texts)
dataset_val = Dataset.from_pandas(val_texts)
dataset_test = Dataset.from_pandas(test_texts)

# Tokenize texts
dataset_train = dataset_train.map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length", max_length=128), batched=True)
dataset_val = dataset_val.map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length", max_length=128), batched=True)
dataset_test = dataset_test.map(lambda x: tokenizer(x["text"], truncation=True, padding="max_length" , max_length=128), batched=True)

dataset_train.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
dataset_val.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])
dataset_test.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])

def create_trainer_train_model(model, train_texts, val_texts, num_epochs):
  def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {"accuracy" : accuracy_score(predictions, labels)}

  args = TrainingArguments(
          output_dir = "content/drive/MyDrive/CryptoFinBert",
          evaluation_strategy = "epoch",
          save_strategy = "epoch",
          learning_rate=2e-5,
          weight_decay=0.01,
          per_device_train_batch_size=32,
          per_device_eval_batch_size=32,
          num_train_epochs=num_epochs,
          load_best_model_at_end=True,
          metric_for_best_model="accuracy",
  )

  trainer = Trainer(
          model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
          args=args,                  # training arguments, defined above
          train_dataset=dataset_train,         # training dataset
          eval_dataset=dataset_val,            # evaluation dataset
          compute_metrics=compute_metrics
  )

  trainer.train()
  return trainer

trainer = create_trainer_train_model(model, dataset_train, dataset_val, num_epochs=3)

def test_model(trainer, test_texts):
  model.eval()
  return trainer.predict(test_texts).metrics

test_model(trainer, dataset_test)

def save_model(trainer, path):
  trainer.save_model(path)

# save_model(trainer, "/CryptoFinBertFinal") #This like is to save the model, but it will overwrite the current model so don't do it
